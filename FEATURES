NeuralNetHack 0.2.4:
	* Parsing:
		- User can now specify an output file suffix for all output files
		  generated by the program.
		- User can specify whether to train a single mlp or an ensemble and
		  also control how big the final ensamle should be.
		- User can now specify in config file which seed to use.
		- Changed the meaning and parsing of EnsParam and MSParam.
	* Data tools:
		- Removal of internal counter for use with next, previous and current
		  Pattern functions.
		- Code cleanup.
		- Rewrite of most routines in DataManager using stl instead of my 
		  own stuff.
		- Bug fixes in the DataManager.
	* Evaluation:
		- AUC calculation based on the mean of N runs of K-fold cross
		  validation.
		- Various minor bug fixes.
		- Possibility of producing an output list containing output for each
		  data point on the training set and testing set using ensembles or
		  single mlp. 
	* Learning Algorithms:
		- Fixed a bug in QuasiNewton and GradientDescent regarding the weight
		  elimination. The biases are now skipped as they should be.
	* Error functions:
		- CrossEntropy always assumes classification problem i.e. it will
		  always consider one output as being a two class problem.
		- Fixed the NaN bug in the CrossEntropy error function.
		- WeightElimination is now performed in the Error interface and not in
		  the Trainer.
	* Misc:
		- Added a Factory for creating Mlp, Trainer. Bootstrapper,
		  CrossValidator etc.
		- Added ErrorMeasures for evaluating a Committee on a DataSet.
		  Evaluations are ROC Area, Summed Square Error, Cross Entropy Error, 
		- Complete rewrite of the ModelSelection part which means that
		  Bootstrapper and CrossValidator methods are implemented for
		  ModelSelection and Bagger and CrossSplitter methods are implemented
		  for EnsembleBuilding.
NeuralNetHack 0.2.3:
	* Error functions:
		- Cross Entropy error function for two-class single output problem.
	* Multi layer perception:
		- Speed up due to the removal of Neurons as a concept.
		- General restructuring of class hierarchy.
	* Evaluation:
		- AUC calculation based on committee output.
		- AUC no longer available for each run and part.
		- ROC analysis on seperatly provided test set.
	* Parsing:
		- Test set is also parsed along with training set now.
		- Various updates.
NeuralNetHack 0.2.2:
	* Learning Algorithms:
		- QuasiNewton:
			- DFP updating rule.
		- Minor bugfixes.
		- Weight decay.
		- Weight elimination.
	* Data tools:
		- Sequential splitting of a data set.
		- Randomised splitting of a data set.
		- Data set normalization.
		- Data set abstraction. 
	* Evaluation:
		- N runs of K-fold crossvalidation:
			- ROC analysis for each run.
			- ROC analysis for all runs.
NeuralNetHack 0.2.1:
	* Error functions:
		- Structure cleanup.
		- Documentation updates.
		- Minor bugfixes.
NeuralNetHack 0.2.0:
	* Learning Algorithms:
		- Quasi Newton:
			- BFGS updating rule.
			- Initial bracketing of the minima.
			- Brents line search.
NeuralNetHack 0.1.0:
	* Multi layer perceptron:
		- Any number of neurons and layers.
		- Tangens Hyperbolic activation function.
		- Sigmoid activation function.
		- Linear activation function.
	* Learning Algorithms:
		- Gradient Descent
			- Variable learning rate.
			- Momentum term aka poor mans conjugate gradient.
		- Block updating.
	* Error functions:
		- Summed Square Error.
	* Evaluation:
		-ROC generation.
		-AUC calculation using mann-whitney or trapezoidal rule.
