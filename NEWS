NeuralNetHack 0.9.0:
	* Config:
		- Introduced a new parameter in the configuration file called Vary
		  that allows a user to specify a parameter to vary in the runs.
		  Currently only weight elimination parameter alpha works.
	* Misc:
		- Made neuralnethack binary capable of
			- GridSearching over weight elimination parameter alpha, and select
			  the best model and train and test that model.
			- Perform a normal cross validation outputting the mean training
			  and validation error.
			- Training and testing.
		- New script to convert a neuralnethack weight vector to the matlab
		  version. Useful for constructing hinton diagrams in matlab.
		- Exporting a full network with all models and the normalization
		  koefficients is now possible. The format is XML based.
		- Importing of a full committee with the normalization koefficints.
		- Lots of more documentation also shifted UML handler.
		- Now using libtool and actually building a library that can be used
		  by a third party.
		- Rewriting of old c-style code to stl library code.
		- Made all parsing routines into a convinience library.
		- Unique identification string in each pattern
	* Evaluation:
		- Implemented the Hosmer-Lemeshow goodness of fit measure.
	* Sampling:
		- Added DummySampler allowing for ensembles based only on different
		  weight initilisations.
		- Added Hold-Out sampler.
		- Sampling via Bootstrapping and Cross-Splitting.
	* Model Selection:
		- Complete rewrite of the Model selection procedure including the
		  ensemble building with the evaluation.
		- Model selection and ensemble building has been further abstracted
		  and now only uses a Sampler to build and evaluate models.
		- Saliency measures on each input variable possible.
		- Effective OddsRatio calculation for an ensemble of Mlps.
	* Learning Algorithms:
		- Block updating now works in GradientDescent.

NeuralNetHack 0.2.4:
	* Parsing:
		- User can now specify an output file suffix for all output files
		  generated by the program.
		- User can specify whether to train a single mlp or an ensemble and
		  also control how big the final ensamle should be.
		- User can now specify in config file which seed to use.
		- Changed the meaning and parsing of EnsParam and MSParam.
	* Data tools:
		- Removal of internal counter for use with next, previous and current
		  Pattern functions.
		- Code cleanup.
		- Rewrite of most routines in DataManager using stl instead of my 
		  own stuff.
		- Bug fixes in the DataManager.
	* Evaluation:
		- AUC calculation based on the mean of N runs of K-fold cross
		  validation.
		- Various minor bug fixes.
		- Possibility of producing an output list containing output for each
		  data point on the training set and testing set using ensembles or
		  single mlp. 
	* Learning Algorithms:
		- Fixed a bug in QuasiNewton and GradientDescent regarding the weight
		  elimination. The biases are now skipped as they should be.
	* Error functions:
		- CrossEntropy always assumes classification problem i.e. it will
		  always consider one output as being a two class problem.
		- Fixed the NaN bug in the CrossEntropy error function.
		- WeightElimination is now performed in the Error interface and not in
		  the Trainer.
	* Misc:
		- Added a Factory for creating Mlp, Trainer. Bootstrapper,
		  CrossValidator etc.
		- Added ErrorMeasures for evaluating a Committee on a DataSet.
		  Evaluations are ROC Area, Summed Square Error, Cross Entropy Error, 
		- Complete rewrite of the ModelSelection part which means that
		  Bootstrapper and CrossValidator methods are implemented for
		  ModelSelection and Bagger and CrossSplitter methods are implemented
		  for EnsembleBuilding.

NeuralNetHack 0.2.3:
	* Error functions:
		- Cross Entropy error function for two-class single output problem.
	* Multi layer perception:
		- Speed up due to the removal of Neurons as a concept.
		- General restructuring of class hierarchy.
	* Evaluation:
		- AUC calculation based on committee output.
		- AUC no longer available for each run and part.
		- ROC analysis on seperatly provided test set.
	* Parsing:
		- Test set is also parsed along with training set now.
		- Various updates.

NeuralNetHack 0.2.2:
	* Learning Algorithms:
		- QuasiNewton:
			- DFP updating rule.
		- Minor bugfixes.
		- Weight decay.
		- Weight elimination.
	* Data tools:
		- Sequential splitting of a data set.
		- Randomised splitting of a data set.
		- Data set normalization.
		- Data set abstraction. 
	* Evaluation:
		- N runs of K-fold crossvalidation:
			- ROC analysis for each run.
			- ROC analysis for all runs.

NeuralNetHack 0.2.1:
	* Error functions:
		- Structure cleanup.
		- Documentation updates.
		- Minor bugfixes.

NeuralNetHack 0.2.0:
	* Learning Algorithms:
		- Quasi Newton:
			- BFGS updating rule.
			- Initial bracketing of the minima.
			- Brents line search.

NeuralNetHack 0.1.0:
	* Multi layer perceptron:
		- Any number of neurons and layers.
		- Tangens Hyperbolic activation function.
		- Sigmoid activation function.
		- Linear activation function.
	* Learning Algorithms:
		- Gradient Descent
			- Variable learning rate.
			- Momentum term aka poor mans conjugate gradient.
		- Block updating.
	* Error functions:
		- Summed Square Error.
	* Evaluation:
		-ROC generation.
		-AUC calculation using mann-whitney or trapezoidal rule.
